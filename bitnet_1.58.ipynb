{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12834598,"sourceType":"datasetVersion","datasetId":8117127},{"sourceId":12834604,"sourceType":"datasetVersion","datasetId":8117132}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport random\nfrom typing import Optional, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"VSjDB67lmIEb","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T07:25:38.918079Z","iopub.execute_input":"2025-08-14T07:25:38.918605Z","iopub.status.idle":"2025-08-14T07:25:45.557352Z","shell.execute_reply.started":"2025-08-14T07:25:38.918575Z","shell.execute_reply":"2025-08-14T07:25:45.556593Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"id":"X6L05gDeofSu","outputId":"e3780797-4493-4d7e-d551-dc3ad5846929","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T07:25:51.966135Z","iopub.execute_input":"2025-08-14T07:25:51.966737Z","iopub.status.idle":"2025-08-14T07:25:52.056602Z","shell.execute_reply.started":"2025-08-14T07:25:51.966711Z","shell.execute_reply":"2025-08-14T07:25:52.055874Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## BITNET IMPLEMENTATION 2","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport time\nimport os\nimport json\nfrom typing import Optional\n\n# Set device and optimize for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Enable mixed precision for faster training\ntorch.backends.cudnn.benchmark = True\n\nclass ImprovedBitLinear(nn.Module):\n    \"\"\"\n    Improved BitNet Linear Layer with better quantization and scaling\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=False):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        # Weight parameter - initialize with Xavier normal\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n        \n        # Optional bias\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_features))\n        else:\n            self.register_parameter('bias', None)\n        \n        # Learnable scaling factors for better gradient flow\n        self.weight_scale = nn.Parameter(torch.ones(1))\n        self.eps = 1e-8\n\n    def quantize_weights(self, w):\n        \"\"\"Improved weight quantization to {-1, 0, +1}\"\"\"\n        # Calculate scale using mean absolute value for stability\n        scale = w.abs().mean().clamp(min=self.eps)\n        \n        # Normalize weights\n        w_norm = w / (scale + self.eps)\n        \n        # Quantize with improved thresholding\n        # Use a learnable threshold or fixed threshold around 0.5\n        threshold = 0.3  # Lower threshold for more active weights\n        w_quant = torch.sign(w_norm) * (w_norm.abs() > threshold).float()\n        \n        return w_quant, scale\n\n    def forward(self, x):\n        # Quantize weights\n        w_quant, w_scale = self.quantize_weights(self.weight)\n        \n        # Improved activation quantization - use tanh for smooth gradients\n        # Instead of hard clipping, use a softer function\n        x_norm = torch.tanh(x)  # Soft activation quantization\n        \n        # Linear transformation\n        output = F.linear(x_norm, w_quant, self.bias)\n        \n        # Apply learned scaling\n        output = output * w_scale * self.weight_scale\n        \n        return output\n\nclass ImprovedBitAttention(nn.Module):\n    \"\"\"Improved BitNet Attention with better stability\"\"\"\n    def __init__(self, dim, heads=4, dropout=0.1):\n        super().__init__()\n        self.heads = heads\n        self.dim = dim\n        self.head_dim = dim // heads\n        self.scale = self.head_dim ** -0.5\n        \n        assert dim % heads == 0, \"dim must be divisible by heads\"\n        \n        # Use separate projections for better learning\n        self.q_proj = ImprovedBitLinear(dim, dim)\n        self.k_proj = ImprovedBitLinear(dim, dim)\n        self.v_proj = ImprovedBitLinear(dim, dim)\n        self.out_proj = ImprovedBitLinear(dim, dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        B, L, D = x.shape\n        \n        # Generate Q, K, V\n        q = self.q_proj(x).view(B, L, self.heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(x).view(B, L, self.heads, self.head_dim).transpose(1, 2)\n        v = self.v_proj(x).view(B, L, self.heads, self.head_dim).transpose(1, 2)\n        \n        # Attention computation with improved stability\n        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # Causal mask\n        mask = torch.tril(torch.ones(L, L, device=x.device, dtype=torch.bool))\n        attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n        \n        # Stable softmax\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        \n        # Handle potential NaN values\n        attn_probs = torch.nan_to_num(attn_probs, nan=0.0)\n        \n        # Apply attention\n        out = (attn_probs @ v).transpose(1, 2).contiguous().view(B, L, D)\n        \n        return self.out_proj(out)\n\nclass ImprovedBitMLP(nn.Module):\n    \"\"\"Improved BitNet MLP with better capacity\"\"\"\n    def __init__(self, dim, expansion_factor=4, dropout=0.1):\n        super().__init__()\n        hidden_dim = int(dim * expansion_factor)\n        \n        self.fc1 = ImprovedBitLinear(dim, hidden_dim)\n        self.fc2 = ImprovedBitLinear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # Use SiLU/Swish activation for better gradients\n        x = self.fc1(x)\n        x = F.silu(x)  # SiLU activation (x * sigmoid(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass ImprovedBitBlock(nn.Module):\n    \"\"\"Improved BitNet Transformer Block\"\"\"\n    def __init__(self, dim, heads=4, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = ImprovedBitAttention(dim, heads, dropout)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        self.mlp = ImprovedBitMLP(dim, expansion_factor=4, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # Pre-norm architecture with residual connections\n        residual = x\n        x = self.norm1(x)\n        x = self.attn(x)\n        x = self.dropout(x)\n        x = x + residual\n        \n        residual = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.dropout(x)\n        x = x + residual\n        \n        return x\n\nclass ImprovedBitNet(nn.Module):\n    \"\"\"Improved BitNet Language Model with better architecture\"\"\"\n    def __init__(self, vocab_size, dim=256, depth=6, heads=8, max_len=512, dropout=0.1):\n        super().__init__()\n        self.dim = dim\n        self.max_len = max_len\n        self.vocab_size = vocab_size\n        \n        # Embeddings with better initialization\n        self.token_emb = nn.Embedding(vocab_size, dim)\n        self.pos_emb = nn.Embedding(max_len, dim)\n        \n        # Input dropout\n        self.emb_dropout = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            ImprovedBitBlock(dim, heads, dropout) \n            for _ in range(depth)\n        ])\n        \n        # Output layers\n        self.norm = nn.LayerNorm(dim, eps=1e-6)\n        self.head = nn.Linear(dim, vocab_size)  # Use regular linear for output\n        \n        # Initialize parameters\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.zeros_(module.bias)\n            torch.nn.init.ones_(module.weight)\n    \n    def forward(self, x, targets=None):\n        B, L = x.shape\n        \n        # Clamp sequence length\n        if L > self.max_len:\n            x = x[:, :self.max_len]\n            if targets is not None:\n                targets = targets[:, :self.max_len]\n            L = self.max_len\n            \n        # Validate input tokens\n        x = torch.clamp(x, 0, self.vocab_size - 1)\n        \n        # Embeddings\n        pos = torch.arange(L, device=x.device)\n        \n        token_embeddings = self.token_emb(x)\n        pos_embeddings = self.pos_emb(pos)\n        \n        x = token_embeddings + pos_embeddings\n        x = self.emb_dropout(x)\n        \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        # Output\n        x = self.norm(x)\n        logits = self.head(x)\n        \n        loss = None\n        if targets is not None:\n            targets = torch.clamp(targets, 0, self.vocab_size - 1)\n            # Label smoothing for better training\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)), \n                targets.view(-1),\n                label_smoothing=0.1\n            )\n            \n        return logits, loss\n\nclass FastTokenizer:\n    \"\"\"Improved tokenizer with better handling\"\"\"\n    def __init__(self, text):\n        # Add special tokens\n        special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n        chars = sorted(set(text))\n        all_tokens = special_tokens + chars\n        \n        self.vocab_size = len(all_tokens)\n        self.char_to_idx = {ch: i for i, ch in enumerate(all_tokens)}\n        self.idx_to_char = {i: ch for i, ch in enumerate(all_tokens)}\n        \n        self.pad_token = 0\n        self.unk_token = 1\n        self.start_token = 2\n        self.end_token = 3\n        \n        print(f\"Vocab size: {self.vocab_size}\")\n        print(f\"Characters: {chars[:20]}...\")\n        \n    def encode(self, text, add_special_tokens=False):\n        tokens = []\n        if add_special_tokens:\n            tokens.append(self.start_token)\n            \n        for c in text:\n            if c in self.char_to_idx:\n                tokens.append(self.char_to_idx[c])\n            else:\n                tokens.append(self.unk_token)\n                \n        if add_special_tokens:\n            tokens.append(self.end_token)\n            \n        return tokens\n    \n    def decode(self, tokens):\n        result = []\n        for t in tokens:\n            if 0 <= t < len(self.idx_to_char):\n                char = self.idx_to_char[t]\n                if char not in ['<pad>', '<unk>', '<start>', '<end>']:\n                    result.append(char)\n        return ''.join(result)\n\nclass FastDataset(Dataset):\n    \"\"\"Improved dataset with better error handling\"\"\"\n    def __init__(self, tokens, seq_len=128):\n        self.tokens = tokens\n        self.seq_len = seq_len\n        \n        if len(tokens) <= seq_len:\n            raise ValueError(f\"Not enough tokens ({len(tokens)}) for sequence length {seq_len}\")\n            \n    def __len__(self):\n        return max(0, len(self.tokens) - self.seq_len)\n    \n    def __getitem__(self, idx):\n        # Ensure we don't go out of bounds\n        if idx + self.seq_len + 1 > len(self.tokens):\n            idx = len(self.tokens) - self.seq_len - 1\n            \n        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n        \n        return x, y\n\ndef train_improved_model(model, dataloader, epochs=5, lr=1e-3):\n    \"\"\"Improved training loop with better optimization\"\"\"\n    model.train()\n    \n    # Better optimizer settings\n    optimizer = torch.optim.AdamW(\n        model.parameters(), \n        lr=lr, \n        betas=(0.9, 0.99), \n        weight_decay=0.01,\n        eps=1e-8\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=epochs, eta_min=1e-6\n    )\n    \n    losses = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        num_batches = len(dataloader)\n        \n        for i, (x, y) in enumerate(dataloader):\n            x, y = x.to(device), y.to(device)\n            \n            optimizer.zero_grad()\n            logits, loss = model(x, y)\n            \n            if torch.isnan(loss):\n                print(f\"NaN loss at epoch {epoch}, batch {i}\")\n                continue\n                \n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n            epoch_loss += loss.item()\n            \n            if i % 50 == 0:\n                print(f'Epoch {epoch+1}/{epochs}, Batch {i}/{num_batches}, '\n                      f'Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n                      \n        scheduler.step()\n        \n        if num_batches > 0:\n            avg_loss = epoch_loss / num_batches\n            losses.append(avg_loss)\n            print(f'Epoch {epoch+1} completed - Avg Loss: {avg_loss:.4f}')\n            \n    return losses\n\ndef generate_improved_text(model, tokenizer, prompt=\"Once upon a time\", max_len=200, temp=0.7, top_p=0.9):\n    \"\"\"Improved text generation with nucleus sampling\"\"\"\n    model.eval()\n    \n    tokens = tokenizer.encode(prompt)\n    if not tokens:\n        tokens = [tokenizer.start_token]\n        \n    generated_tokens = tokens.copy()\n    \n    with torch.no_grad():\n        for _ in range(max_len):\n            # Prepare input\n            input_tokens = generated_tokens[-model.max_len:] if len(generated_tokens) > model.max_len else generated_tokens\n            x = torch.tensor([input_tokens], device=device)\n            \n            # Get logits\n            logits, _ = model(x)\n            next_token_logits = logits[0, -1] / temp\n            \n            # Apply nucleus (top-p) sampling\n            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n            \n            # Remove tokens with cumulative probability above the threshold\n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n            sorted_indices_to_remove[0] = 0\n            \n            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n            next_token_logits[indices_to_remove] = float('-inf')\n            \n            # Sample next token\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1).item()\n            \n            # Check for end token or padding\n            if next_token == tokenizer.end_token:\n                break\n                \n            generated_tokens.append(next_token)\n            \n            # Prevent infinite generation\n            if len(generated_tokens) > model.max_len * 2:\n                break\n                \n    return tokenizer.decode(generated_tokens)\n\ndef load_data():\n    \"\"\"Load data with fallback to sample text\"\"\"\n    try:\n        with open('/kaggle/input/holly-songs/iLoveMerge.txt', 'r', encoding='utf-8') as f:\n            text = f.read()\n        print(f\"Loaded {len(text)} characters from file\")\n    except FileNotFoundError:\n        print(\"Data file not found, using sample text\")\n        # Use a more coherent sample text\n        text = \"\"\"\n        Once upon a time, in a land far away, there lived a young princess named Luna. \n        She had long silver hair that sparkled like starlight and eyes as blue as the deepest ocean.\n        Every day, she would walk through the enchanted forest near her castle, \n        talking to the woodland creatures and learning their ancient wisdom.\n        \n        The forest was magical, filled with talking animals, glowing flowers, and trees that sang lullabies.\n        Luna's favorite spot was a small clearing where a crystal-clear stream bubbled over smooth stones.\n        Here, she would sit and practice her magic, creating beautiful illusions of butterflies and rainbows.\n        \n        One day, while practicing her spells, Luna discovered she had a special gift.\n        She could understand and speak the language of all living things.\n        The flowers whispered secrets about the weather, the birds shared news from distant lands,\n        and the old oak tree told stories of heroes from long ago.\n        \n        With this new ability, Luna realized she could help bring peace to her kingdom.\n        She traveled from village to village, solving conflicts between humans and nature,\n        teaching people to live in harmony with the world around them.\n        \n        As the years passed, Luna became known as the Harmony Princess,\n        beloved by all creatures great and small. Her kingdom flourished,\n        becoming a place where magic and nature lived in perfect balance.\n        \n        And they all lived happily ever after, in a world filled with wonder and joy.\n        \"\"\" * 20  # Repeat to have more training data\n        \n    return text\n\ndef main():\n    print(\"⚡ Improved BitNet Language Model ⚡\")\n    print(\"=\" * 50)\n    \n    # Load data\n    text = load_data()\n    \n    # Create tokenizer and dataset\n    tokenizer = FastTokenizer(text)\n    tokens = tokenizer.encode(text)\n    \n    print(f\"Total tokens: {len(tokens)}\")\n    print(f\"Vocab size: {tokenizer.vocab_size}\")\n    \n    # Create dataset\n    try:\n        dataset = FastDataset(tokens, seq_len=192)\n        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n        print(f\"Dataset created: {len(dataset)} sequences\")\n    except ValueError as e:\n        print(f\"Error creating dataset: {e}\")\n        return\n    \n    # Create improved model\n    model = ImprovedBitNet(\n        vocab_size=tokenizer.vocab_size,\n        dim=384,  # Reasonable size\n        depth=6,   # Fewer layers but better quality\n        heads=6,\n        max_len=256,\n        dropout=0.05\n    ).to(device)\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Model parameters: {total_params:,}\")\n    \n    # Train model\n    print(\"\\n🚀 Starting training...\")\n    losses = train_improved_model(model, dataloader, epochs=15, lr=2e-4)\n    \n    # Plot losses\n    if losses:\n        plt.figure(figsize=(10, 6))\n        plt.plot(losses, 'b-', linewidth=2, marker='o')\n        plt.title('Improved BitNet Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n    \n    # Generate text samples\n    print(\"\\n📝 Generating improved text samples...\")\n    model.eval()\n    \n    prompts = [\n        \"Once upon a time\",\n        \"In a magical forest\",\n        \"The princess discovered\",\n        \"Long ago, there lived\",\n        \"Every day, she would\"\n    ]\n    \n    for prompt in prompts:\n        print(f\"\\n🌟 Prompt: '{prompt}'\")\n        generated = generate_improved_text(\n            model, tokenizer, prompt, \n            max_len=150, temp=0.7, top_p=0.9\n        )\n        print(f\"Generated: {generated}\")\n        print(\"-\" * 60)\n    \n    print(\"\\n✅ Improved BitNet training and generation completed!\")\n    \n    # Model statistics\n    bitnet_params = sum(p.numel() for p in model.parameters() if isinstance(p, torch.nn.Parameter))\n    estimated_size = bitnet_params * 1.58 / 8 / (1024 * 1024)  # BitNet compression\n    regular_size = bitnet_params * 32 / 8 / (1024 * 1024)\n    \n    print(f\"\\n📊 Model Statistics:\")\n    print(f\"Parameters: {bitnet_params:,}\")\n    print(f\"Estimated BitNet size: {estimated_size:.1f} MB\")\n    print(f\"Regular model size: {regular_size:.1f} MB\")\n    print(f\"Compression ratio: {regular_size/estimated_size:.1f}x\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:19:16.845283Z","iopub.execute_input":"2025-09-01T08:19:16.845896Z","iopub.status.idle":"2025-09-01T09:12:57.470660Z","shell.execute_reply.started":"2025-09-01T08:19:16.845874Z","shell.execute_reply":"2025-09-01T09:12:57.469839Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n⚡ Improved BitNet Language Model ⚡\n==================================================\nData file not found, using sample text\nVocab size: 43\nCharacters: ['\\n', ' ', \"'\", ',', '-', '.', 'A', 'E', 'H', 'L', 'O', 'P', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e']...\nTotal tokens: 32180\nVocab size: 43\nDataset created: 31988 sequences\nModel parameters: 10,758,223\n\n🚀 Starting training...\nEpoch 1/15, Batch 0/1000, Loss: 3.8146, LR: 0.000200\nEpoch 1/15, Batch 50/1000, Loss: 2.9789, LR: 0.000200\nEpoch 1/15, Batch 100/1000, Loss: 2.5591, LR: 0.000200\nEpoch 1/15, Batch 150/1000, Loss: 2.4849, LR: 0.000200\nEpoch 1/15, Batch 200/1000, Loss: 2.4243, LR: 0.000200\nEpoch 1/15, Batch 250/1000, Loss: 2.3497, LR: 0.000200\nEpoch 1/15, Batch 300/1000, Loss: 2.2886, LR: 0.000200\nEpoch 1/15, Batch 350/1000, Loss: 2.2004, LR: 0.000200\nEpoch 1/15, Batch 400/1000, Loss: 2.1493, LR: 0.000200\nEpoch 1/15, Batch 450/1000, Loss: 2.1434, LR: 0.000200\nEpoch 1/15, Batch 500/1000, Loss: 2.0540, LR: 0.000200\nEpoch 1/15, Batch 550/1000, Loss: 1.9963, LR: 0.000200\nEpoch 1/15, Batch 600/1000, Loss: 1.9485, LR: 0.000200\nEpoch 1/15, Batch 650/1000, Loss: 1.9191, LR: 0.000200\nEpoch 1/15, Batch 700/1000, Loss: 1.8783, LR: 0.000200\nEpoch 1/15, Batch 750/1000, Loss: 1.8462, LR: 0.000200\nEpoch 1/15, Batch 800/1000, Loss: 1.7814, LR: 0.000200\nEpoch 1/15, Batch 850/1000, Loss: 1.7239, LR: 0.000200\nEpoch 1/15, Batch 900/1000, Loss: 1.7078, LR: 0.000200\nEpoch 1/15, Batch 950/1000, Loss: 1.6770, LR: 0.000200\nEpoch 1 completed - Avg Loss: 2.1273\nEpoch 2/15, Batch 0/1000, Loss: 1.6468, LR: 0.000198\nEpoch 2/15, Batch 50/1000, Loss: 1.6287, LR: 0.000198\nEpoch 2/15, Batch 100/1000, Loss: 1.6318, LR: 0.000198\nEpoch 2/15, Batch 150/1000, Loss: 1.6002, LR: 0.000198\nEpoch 2/15, Batch 200/1000, Loss: 1.5634, LR: 0.000198\nEpoch 2/15, Batch 250/1000, Loss: 1.5726, LR: 0.000198\nEpoch 2/15, Batch 300/1000, Loss: 1.5425, LR: 0.000198\nEpoch 2/15, Batch 350/1000, Loss: 1.5111, LR: 0.000198\nEpoch 2/15, Batch 400/1000, Loss: 1.5152, LR: 0.000198\nEpoch 2/15, Batch 450/1000, Loss: 1.4741, LR: 0.000198\nEpoch 2/15, Batch 500/1000, Loss: 1.4702, LR: 0.000198\nEpoch 2/15, Batch 550/1000, Loss: 1.4249, LR: 0.000198\nEpoch 2/15, Batch 600/1000, Loss: 1.4928, LR: 0.000198\nEpoch 2/15, Batch 650/1000, Loss: 1.4329, LR: 0.000198\nEpoch 2/15, Batch 700/1000, Loss: 1.4107, LR: 0.000198\nEpoch 2/15, Batch 750/1000, Loss: 1.4418, LR: 0.000198\nEpoch 2/15, Batch 800/1000, Loss: 1.3967, LR: 0.000198\nEpoch 2/15, Batch 850/1000, Loss: 1.4015, LR: 0.000198\nEpoch 2/15, Batch 900/1000, Loss: 1.3990, LR: 0.000198\nEpoch 2/15, Batch 950/1000, Loss: 1.3856, LR: 0.000198\nEpoch 2 completed - Avg Loss: 1.4899\nEpoch 3/15, Batch 0/1000, Loss: 1.3926, LR: 0.000191\nEpoch 3/15, Batch 50/1000, Loss: 1.3566, LR: 0.000191\nEpoch 3/15, Batch 100/1000, Loss: 1.3774, LR: 0.000191\nEpoch 3/15, Batch 150/1000, Loss: 1.3708, LR: 0.000191\nEpoch 3/15, Batch 200/1000, Loss: 1.3547, LR: 0.000191\nEpoch 3/15, Batch 250/1000, Loss: 1.3432, LR: 0.000191\nEpoch 3/15, Batch 300/1000, Loss: 1.3205, LR: 0.000191\nEpoch 3/15, Batch 350/1000, Loss: 1.3275, LR: 0.000191\nEpoch 3/15, Batch 400/1000, Loss: 1.3227, LR: 0.000191\nEpoch 3/15, Batch 450/1000, Loss: 1.3307, LR: 0.000191\nEpoch 3/15, Batch 500/1000, Loss: 1.3112, LR: 0.000191\nEpoch 3/15, Batch 550/1000, Loss: 1.3045, LR: 0.000191\nEpoch 3/15, Batch 600/1000, Loss: 1.3102, LR: 0.000191\nEpoch 3/15, Batch 650/1000, Loss: 1.3081, LR: 0.000191\nEpoch 3/15, Batch 700/1000, Loss: 1.2916, LR: 0.000191\nEpoch 3/15, Batch 750/1000, Loss: 1.2725, LR: 0.000191\nEpoch 3/15, Batch 800/1000, Loss: 1.2830, LR: 0.000191\nEpoch 3/15, Batch 850/1000, Loss: 1.2704, LR: 0.000191\nEpoch 3/15, Batch 900/1000, Loss: 1.2774, LR: 0.000191\nEpoch 3/15, Batch 950/1000, Loss: 1.3043, LR: 0.000191\nEpoch 3 completed - Avg Loss: 1.3186\nEpoch 4/15, Batch 0/1000, Loss: 1.2581, LR: 0.000181\nEpoch 4/15, Batch 50/1000, Loss: 1.2490, LR: 0.000181\nEpoch 4/15, Batch 100/1000, Loss: 1.2839, LR: 0.000181\nEpoch 4/15, Batch 150/1000, Loss: 1.2897, LR: 0.000181\nEpoch 4/15, Batch 200/1000, Loss: 1.2718, LR: 0.000181\nEpoch 4/15, Batch 250/1000, Loss: 1.2569, LR: 0.000181\nEpoch 4/15, Batch 300/1000, Loss: 1.2664, LR: 0.000181\nEpoch 4/15, Batch 350/1000, Loss: 1.2431, LR: 0.000181\nEpoch 4/15, Batch 400/1000, Loss: 1.2556, LR: 0.000181\nEpoch 4/15, Batch 450/1000, Loss: 1.2396, LR: 0.000181\nEpoch 4/15, Batch 500/1000, Loss: 1.2311, LR: 0.000181\nEpoch 4/15, Batch 550/1000, Loss: 1.2308, LR: 0.000181\nEpoch 4/15, Batch 600/1000, Loss: 1.2371, LR: 0.000181\nEpoch 4/15, Batch 650/1000, Loss: 1.2011, LR: 0.000181\nEpoch 4/15, Batch 700/1000, Loss: 1.2216, LR: 0.000181\nEpoch 4/15, Batch 750/1000, Loss: 1.2102, LR: 0.000181\nEpoch 4/15, Batch 800/1000, Loss: 1.2368, LR: 0.000181\nEpoch 4/15, Batch 850/1000, Loss: 1.2111, LR: 0.000181\nEpoch 4/15, Batch 900/1000, Loss: 1.2177, LR: 0.000181\nEpoch 4/15, Batch 950/1000, Loss: 1.2197, LR: 0.000181\nEpoch 4 completed - Avg Loss: 1.2412\nEpoch 5/15, Batch 0/1000, Loss: 1.2217, LR: 0.000167\nEpoch 5/15, Batch 50/1000, Loss: 1.2132, LR: 0.000167\nEpoch 5/15, Batch 100/1000, Loss: 1.2224, LR: 0.000167\nEpoch 5/15, Batch 150/1000, Loss: 1.1941, LR: 0.000167\nEpoch 5/15, Batch 200/1000, Loss: 1.2227, LR: 0.000167\nEpoch 5/15, Batch 250/1000, Loss: 1.2085, LR: 0.000167\nEpoch 5/15, Batch 300/1000, Loss: 1.2094, LR: 0.000167\nEpoch 5/15, Batch 350/1000, Loss: 1.1980, LR: 0.000167\nEpoch 5/15, Batch 400/1000, Loss: 1.2116, LR: 0.000167\nEpoch 5/15, Batch 450/1000, Loss: 1.2022, LR: 0.000167\nEpoch 5/15, Batch 500/1000, Loss: 1.2090, LR: 0.000167\nEpoch 5/15, Batch 550/1000, Loss: 1.1859, LR: 0.000167\nEpoch 5/15, Batch 600/1000, Loss: 1.2063, LR: 0.000167\nEpoch 5/15, Batch 650/1000, Loss: 1.1867, LR: 0.000167\nEpoch 5/15, Batch 700/1000, Loss: 1.1723, LR: 0.000167\nEpoch 5/15, Batch 750/1000, Loss: 1.1928, LR: 0.000167\nEpoch 5/15, Batch 800/1000, Loss: 1.1874, LR: 0.000167\nEpoch 5/15, Batch 850/1000, Loss: 1.1939, LR: 0.000167\nEpoch 5/15, Batch 900/1000, Loss: 1.1949, LR: 0.000167\nEpoch 5/15, Batch 950/1000, Loss: 1.1680, LR: 0.000167\nEpoch 5 completed - Avg Loss: 1.2002\nEpoch 6/15, Batch 0/1000, Loss: 1.2011, LR: 0.000150\nEpoch 6/15, Batch 50/1000, Loss: 1.1772, LR: 0.000150\nEpoch 6/15, Batch 100/1000, Loss: 1.1721, LR: 0.000150\nEpoch 6/15, Batch 150/1000, Loss: 1.1874, LR: 0.000150\nEpoch 6/15, Batch 200/1000, Loss: 1.1810, LR: 0.000150\nEpoch 6/15, Batch 250/1000, Loss: 1.1993, LR: 0.000150\nEpoch 6/15, Batch 300/1000, Loss: 1.1986, LR: 0.000150\nEpoch 6/15, Batch 350/1000, Loss: 1.1906, LR: 0.000150\nEpoch 6/15, Batch 400/1000, Loss: 1.1816, LR: 0.000150\nEpoch 6/15, Batch 450/1000, Loss: 1.1843, LR: 0.000150\nEpoch 6/15, Batch 500/1000, Loss: 1.1677, LR: 0.000150\nEpoch 6/15, Batch 550/1000, Loss: 1.1671, LR: 0.000150\nEpoch 6/15, Batch 600/1000, Loss: 1.1684, LR: 0.000150\nEpoch 6/15, Batch 650/1000, Loss: 1.1613, LR: 0.000150\nEpoch 6/15, Batch 700/1000, Loss: 1.1641, LR: 0.000150\nEpoch 6/15, Batch 750/1000, Loss: 1.1705, LR: 0.000150\nEpoch 6/15, Batch 800/1000, Loss: 1.1789, LR: 0.000150\nEpoch 6/15, Batch 850/1000, Loss: 1.1750, LR: 0.000150\nEpoch 6/15, Batch 900/1000, Loss: 1.1590, LR: 0.000150\nEpoch 6/15, Batch 950/1000, Loss: 1.1570, LR: 0.000150\nEpoch 6 completed - Avg Loss: 1.1745\nEpoch 7/15, Batch 0/1000, Loss: 1.1604, LR: 0.000131\nEpoch 7/15, Batch 50/1000, Loss: 1.1710, LR: 0.000131\nEpoch 7/15, Batch 100/1000, Loss: 1.1599, LR: 0.000131\nEpoch 7/15, Batch 150/1000, Loss: 1.1496, LR: 0.000131\nEpoch 7/15, Batch 200/1000, Loss: 1.1531, LR: 0.000131\nEpoch 7/15, Batch 250/1000, Loss: 1.1374, LR: 0.000131\nEpoch 7/15, Batch 300/1000, Loss: 1.1817, LR: 0.000131\nEpoch 7/15, Batch 350/1000, Loss: 1.1518, LR: 0.000131\nEpoch 7/15, Batch 400/1000, Loss: 1.1476, LR: 0.000131\nEpoch 7/15, Batch 450/1000, Loss: 1.1447, LR: 0.000131\nEpoch 7/15, Batch 500/1000, Loss: 1.1537, LR: 0.000131\nEpoch 7/15, Batch 550/1000, Loss: 1.1401, LR: 0.000131\nEpoch 7/15, Batch 600/1000, Loss: 1.1473, LR: 0.000131\nEpoch 7/15, Batch 650/1000, Loss: 1.1569, LR: 0.000131\nEpoch 7/15, Batch 700/1000, Loss: 1.1666, LR: 0.000131\nEpoch 7/15, Batch 750/1000, Loss: 1.1635, LR: 0.000131\nEpoch 7/15, Batch 800/1000, Loss: 1.1551, LR: 0.000131\nEpoch 7/15, Batch 850/1000, Loss: 1.1388, LR: 0.000131\nEpoch 7/15, Batch 900/1000, Loss: 1.1563, LR: 0.000131\nEpoch 7/15, Batch 950/1000, Loss: 1.1614, LR: 0.000131\nEpoch 7 completed - Avg Loss: 1.1559\nEpoch 8/15, Batch 0/1000, Loss: 1.1348, LR: 0.000111\nEpoch 8/15, Batch 50/1000, Loss: 1.1466, LR: 0.000111\nEpoch 8/15, Batch 100/1000, Loss: 1.1680, LR: 0.000111\nEpoch 8/15, Batch 150/1000, Loss: 1.1569, LR: 0.000111\nEpoch 8/15, Batch 200/1000, Loss: 1.1311, LR: 0.000111\nEpoch 8/15, Batch 250/1000, Loss: 1.1622, LR: 0.000111\nEpoch 8/15, Batch 300/1000, Loss: 1.1365, LR: 0.000111\nEpoch 8/15, Batch 350/1000, Loss: 1.1485, LR: 0.000111\nEpoch 8/15, Batch 400/1000, Loss: 1.1503, LR: 0.000111\nEpoch 8/15, Batch 450/1000, Loss: 1.1285, LR: 0.000111\nEpoch 8/15, Batch 500/1000, Loss: 1.1604, LR: 0.000111\nEpoch 8/15, Batch 550/1000, Loss: 1.1281, LR: 0.000111\nEpoch 8/15, Batch 600/1000, Loss: 1.1472, LR: 0.000111\nEpoch 8/15, Batch 650/1000, Loss: 1.1399, LR: 0.000111\nEpoch 8/15, Batch 700/1000, Loss: 1.1549, LR: 0.000111\nEpoch 8/15, Batch 750/1000, Loss: 1.1386, LR: 0.000111\nEpoch 8/15, Batch 800/1000, Loss: 1.1552, LR: 0.000111\nEpoch 8/15, Batch 850/1000, Loss: 1.1384, LR: 0.000111\nEpoch 8/15, Batch 900/1000, Loss: 1.1459, LR: 0.000111\nEpoch 8/15, Batch 950/1000, Loss: 1.1494, LR: 0.000111\nEpoch 8 completed - Avg Loss: 1.1433\nEpoch 9/15, Batch 0/1000, Loss: 1.1556, LR: 0.000090\nEpoch 9/15, Batch 50/1000, Loss: 1.1477, LR: 0.000090\nEpoch 9/15, Batch 100/1000, Loss: 1.1392, LR: 0.000090\nEpoch 9/15, Batch 150/1000, Loss: 1.1440, LR: 0.000090\nEpoch 9/15, Batch 200/1000, Loss: 1.1411, LR: 0.000090\nEpoch 9/15, Batch 250/1000, Loss: 1.1212, LR: 0.000090\nEpoch 9/15, Batch 300/1000, Loss: 1.1368, LR: 0.000090\nEpoch 9/15, Batch 350/1000, Loss: 1.1330, LR: 0.000090\nEpoch 9/15, Batch 400/1000, Loss: 1.1530, LR: 0.000090\nEpoch 9/15, Batch 450/1000, Loss: 1.1362, LR: 0.000090\nEpoch 9/15, Batch 500/1000, Loss: 1.1330, LR: 0.000090\nEpoch 9/15, Batch 550/1000, Loss: 1.1195, LR: 0.000090\nEpoch 9/15, Batch 600/1000, Loss: 1.1422, LR: 0.000090\nEpoch 9/15, Batch 650/1000, Loss: 1.1252, LR: 0.000090\nEpoch 9/15, Batch 700/1000, Loss: 1.1335, LR: 0.000090\nEpoch 9/15, Batch 750/1000, Loss: 1.1322, LR: 0.000090\nEpoch 9/15, Batch 800/1000, Loss: 1.1249, LR: 0.000090\nEpoch 9/15, Batch 850/1000, Loss: 1.1339, LR: 0.000090\nEpoch 9/15, Batch 900/1000, Loss: 1.1334, LR: 0.000090\nEpoch 9/15, Batch 950/1000, Loss: 1.1310, LR: 0.000090\nEpoch 9 completed - Avg Loss: 1.1332\nEpoch 10/15, Batch 0/1000, Loss: 1.1205, LR: 0.000070\nEpoch 10/15, Batch 50/1000, Loss: 1.1150, LR: 0.000070\nEpoch 10/15, Batch 100/1000, Loss: 1.1309, LR: 0.000070\nEpoch 10/15, Batch 150/1000, Loss: 1.1364, LR: 0.000070\nEpoch 10/15, Batch 200/1000, Loss: 1.1214, LR: 0.000070\nEpoch 10/15, Batch 250/1000, Loss: 1.1324, LR: 0.000070\nEpoch 10/15, Batch 300/1000, Loss: 1.1211, LR: 0.000070\nEpoch 10/15, Batch 350/1000, Loss: 1.1266, LR: 0.000070\nEpoch 10/15, Batch 400/1000, Loss: 1.1217, LR: 0.000070\nEpoch 10/15, Batch 450/1000, Loss: 1.1376, LR: 0.000070\nEpoch 10/15, Batch 500/1000, Loss: 1.1334, LR: 0.000070\nEpoch 10/15, Batch 550/1000, Loss: 1.1321, LR: 0.000070\nEpoch 10/15, Batch 600/1000, Loss: 1.1259, LR: 0.000070\nEpoch 10/15, Batch 650/1000, Loss: 1.1208, LR: 0.000070\nEpoch 10/15, Batch 700/1000, Loss: 1.1265, LR: 0.000070\nEpoch 10/15, Batch 750/1000, Loss: 1.1195, LR: 0.000070\nEpoch 10/15, Batch 800/1000, Loss: 1.1264, LR: 0.000070\nEpoch 10/15, Batch 850/1000, Loss: 1.1200, LR: 0.000070\nEpoch 10/15, Batch 900/1000, Loss: 1.1182, LR: 0.000070\nEpoch 10/15, Batch 950/1000, Loss: 1.1217, LR: 0.000070\nEpoch 10 completed - Avg Loss: 1.1257\nEpoch 11/15, Batch 0/1000, Loss: 1.1196, LR: 0.000051\nEpoch 11/15, Batch 50/1000, Loss: 1.1255, LR: 0.000051\nEpoch 11/15, Batch 100/1000, Loss: 1.1133, LR: 0.000051\nEpoch 11/15, Batch 150/1000, Loss: 1.1315, LR: 0.000051\nEpoch 11/15, Batch 200/1000, Loss: 1.1226, LR: 0.000051\nEpoch 11/15, Batch 250/1000, Loss: 1.1109, LR: 0.000051\nEpoch 11/15, Batch 300/1000, Loss: 1.1284, LR: 0.000051\nEpoch 11/15, Batch 350/1000, Loss: 1.1336, LR: 0.000051\nEpoch 11/15, Batch 400/1000, Loss: 1.1174, LR: 0.000051\nEpoch 11/15, Batch 450/1000, Loss: 1.1262, LR: 0.000051\nEpoch 11/15, Batch 500/1000, Loss: 1.1255, LR: 0.000051\nEpoch 11/15, Batch 550/1000, Loss: 1.1205, LR: 0.000051\nEpoch 11/15, Batch 600/1000, Loss: 1.1126, LR: 0.000051\nEpoch 11/15, Batch 650/1000, Loss: 1.1287, LR: 0.000051\nEpoch 11/15, Batch 700/1000, Loss: 1.1203, LR: 0.000051\nEpoch 11/15, Batch 750/1000, Loss: 1.1187, LR: 0.000051\nEpoch 11/15, Batch 800/1000, Loss: 1.1149, LR: 0.000051\nEpoch 11/15, Batch 850/1000, Loss: 1.1038, LR: 0.000051\nEpoch 11/15, Batch 900/1000, Loss: 1.1153, LR: 0.000051\nEpoch 11/15, Batch 950/1000, Loss: 1.1104, LR: 0.000051\nEpoch 11 completed - Avg Loss: 1.1201\nEpoch 12/15, Batch 0/1000, Loss: 1.1156, LR: 0.000034\nEpoch 12/15, Batch 50/1000, Loss: 1.1078, LR: 0.000034\nEpoch 12/15, Batch 100/1000, Loss: 1.1139, LR: 0.000034\nEpoch 12/15, Batch 150/1000, Loss: 1.1312, LR: 0.000034\nEpoch 12/15, Batch 200/1000, Loss: 1.1256, LR: 0.000034\nEpoch 12/15, Batch 250/1000, Loss: 1.1160, LR: 0.000034\nEpoch 12/15, Batch 300/1000, Loss: 1.1135, LR: 0.000034\nEpoch 12/15, Batch 350/1000, Loss: 1.1268, LR: 0.000034\nEpoch 12/15, Batch 400/1000, Loss: 1.1157, LR: 0.000034\nEpoch 12/15, Batch 450/1000, Loss: 1.1143, LR: 0.000034\nEpoch 12/15, Batch 500/1000, Loss: 1.1023, LR: 0.000034\nEpoch 12/15, Batch 550/1000, Loss: 1.1048, LR: 0.000034\nEpoch 12/15, Batch 600/1000, Loss: 1.1029, LR: 0.000034\nEpoch 12/15, Batch 650/1000, Loss: 1.1289, LR: 0.000034\nEpoch 12/15, Batch 700/1000, Loss: 1.1215, LR: 0.000034\nEpoch 12/15, Batch 750/1000, Loss: 1.1132, LR: 0.000034\nEpoch 12/15, Batch 800/1000, Loss: 1.1207, LR: 0.000034\nEpoch 12/15, Batch 850/1000, Loss: 1.1186, LR: 0.000034\nEpoch 12/15, Batch 900/1000, Loss: 1.1256, LR: 0.000034\nEpoch 12/15, Batch 950/1000, Loss: 1.1111, LR: 0.000034\nEpoch 12 completed - Avg Loss: 1.1159\nEpoch 13/15, Batch 0/1000, Loss: 1.1215, LR: 0.000020\nEpoch 13/15, Batch 50/1000, Loss: 1.1304, LR: 0.000020\nEpoch 13/15, Batch 100/1000, Loss: 1.0971, LR: 0.000020\nEpoch 13/15, Batch 150/1000, Loss: 1.1137, LR: 0.000020\nEpoch 13/15, Batch 200/1000, Loss: 1.1441, LR: 0.000020\nEpoch 13/15, Batch 250/1000, Loss: 1.1308, LR: 0.000020\nEpoch 13/15, Batch 300/1000, Loss: 1.1276, LR: 0.000020\nEpoch 13/15, Batch 350/1000, Loss: 1.1352, LR: 0.000020\nEpoch 13/15, Batch 400/1000, Loss: 1.1217, LR: 0.000020\nEpoch 13/15, Batch 450/1000, Loss: 1.1215, LR: 0.000020\nEpoch 13/15, Batch 500/1000, Loss: 1.1297, LR: 0.000020\nEpoch 13/15, Batch 550/1000, Loss: 1.1076, LR: 0.000020\nEpoch 13/15, Batch 600/1000, Loss: 1.1169, LR: 0.000020\nEpoch 13/15, Batch 650/1000, Loss: 1.0895, LR: 0.000020\nEpoch 13/15, Batch 700/1000, Loss: 1.1233, LR: 0.000020\nEpoch 13/15, Batch 750/1000, Loss: 1.1078, LR: 0.000020\nEpoch 13/15, Batch 800/1000, Loss: 1.1098, LR: 0.000020\nEpoch 13/15, Batch 850/1000, Loss: 1.1264, LR: 0.000020\nEpoch 13/15, Batch 900/1000, Loss: 1.1148, LR: 0.000020\nEpoch 13/15, Batch 950/1000, Loss: 1.1237, LR: 0.000020\nEpoch 13 completed - Avg Loss: 1.1136\nEpoch 14/15, Batch 0/1000, Loss: 1.1130, LR: 0.000010\nEpoch 14/15, Batch 50/1000, Loss: 1.1197, LR: 0.000010\nEpoch 14/15, Batch 100/1000, Loss: 1.1212, LR: 0.000010\nEpoch 14/15, Batch 150/1000, Loss: 1.1024, LR: 0.000010\nEpoch 14/15, Batch 200/1000, Loss: 1.1072, LR: 0.000010\nEpoch 14/15, Batch 250/1000, Loss: 1.1107, LR: 0.000010\nEpoch 14/15, Batch 300/1000, Loss: 1.1225, LR: 0.000010\nEpoch 14/15, Batch 350/1000, Loss: 1.1080, LR: 0.000010\nEpoch 14/15, Batch 400/1000, Loss: 1.1000, LR: 0.000010\nEpoch 14/15, Batch 450/1000, Loss: 1.1007, LR: 0.000010\nEpoch 14/15, Batch 500/1000, Loss: 1.1055, LR: 0.000010\nEpoch 14/15, Batch 550/1000, Loss: 1.0974, LR: 0.000010\nEpoch 14/15, Batch 600/1000, Loss: 1.1256, LR: 0.000010\nEpoch 14/15, Batch 650/1000, Loss: 1.1254, LR: 0.000010\nEpoch 14/15, Batch 700/1000, Loss: 1.1260, LR: 0.000010\nEpoch 14/15, Batch 750/1000, Loss: 1.1008, LR: 0.000010\nEpoch 14/15, Batch 800/1000, Loss: 1.0956, LR: 0.000010\nEpoch 14/15, Batch 850/1000, Loss: 1.1105, LR: 0.000010\nEpoch 14/15, Batch 900/1000, Loss: 1.1180, LR: 0.000010\nEpoch 14/15, Batch 950/1000, Loss: 1.1223, LR: 0.000010\nEpoch 14 completed - Avg Loss: 1.1114\nEpoch 15/15, Batch 0/1000, Loss: 1.1211, LR: 0.000003\nEpoch 15/15, Batch 50/1000, Loss: 1.1178, LR: 0.000003\nEpoch 15/15, Batch 100/1000, Loss: 1.1232, LR: 0.000003\nEpoch 15/15, Batch 150/1000, Loss: 1.1027, LR: 0.000003\nEpoch 15/15, Batch 200/1000, Loss: 1.1222, LR: 0.000003\nEpoch 15/15, Batch 250/1000, Loss: 1.1248, LR: 0.000003\nEpoch 15/15, Batch 300/1000, Loss: 1.0968, LR: 0.000003\nEpoch 15/15, Batch 350/1000, Loss: 1.0971, LR: 0.000003\nEpoch 15/15, Batch 400/1000, Loss: 1.1209, LR: 0.000003\nEpoch 15/15, Batch 450/1000, Loss: 1.1117, LR: 0.000003\nEpoch 15/15, Batch 500/1000, Loss: 1.0902, LR: 0.000003\nEpoch 15/15, Batch 550/1000, Loss: 1.1008, LR: 0.000003\nEpoch 15/15, Batch 600/1000, Loss: 1.1234, LR: 0.000003\nEpoch 15/15, Batch 650/1000, Loss: 1.1043, LR: 0.000003\nEpoch 15/15, Batch 700/1000, Loss: 1.1096, LR: 0.000003\nEpoch 15/15, Batch 750/1000, Loss: 1.1216, LR: 0.000003\nEpoch 15/15, Batch 800/1000, Loss: 1.1031, LR: 0.000003\nEpoch 15/15, Batch 850/1000, Loss: 1.1208, LR: 0.000003\nEpoch 15/15, Batch 900/1000, Loss: 1.1102, LR: 0.000003\nEpoch 15/15, Batch 950/1000, Loss: 1.1198, LR: 0.000003\nEpoch 15 completed - Avg Loss: 1.1109\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n📝 Generating improved text samples...\n\n🌟 Prompt: 'Once upon a time'\nGenerated: Once upon a time, in in a land the oprm in lag ceanature lived in in perfects baland tures and thepest ocean.\n        Every day, she would walk through the enchanted \n------------------------------------------------------------\n\n🌟 Prompt: 'In a magical forest'\nGenerated: n a magical forestand lar cretulies and learng their ancient wisdom.\n        \n        The forest was magical, filed with talking animals, glowing flowers, and treeees t\n------------------------------------------------------------\n\n🌟 Prompt: 'The princess discovered'\nGenerated: The princess discovered haby crestarestand small.\n        \n        One day, while practicing her spells, Luna discovered she had a special gift.\n        She could understand\n------------------------------------------------------------\n\n🌟 Prompt: 'Long ago, there lived'\nGenerated: Long ago, there lived spracing he ha witat-cler s s, soled shered soulersthes bubers shauthed news fhom long ag ag pleactonce weathere tond oar stresthe sare, the birds sh\n------------------------------------------------------------\n\n🌟 Prompt: 'Every day, she would'\nGenerated: Every day, she would walk througd the the wondesthener arom ld f lorom long ag lo.\n        \n        \n        Withis new ability, Luna realized she could help bring peace \n------------------------------------------------------------\n\n✅ Improved BitNet training and generation completed!\n\n📊 Model Statistics:\nParameters: 10,758,223\nEstimated BitNet size: 2.0 MB\nRegular model size: 41.0 MB\nCompression ratio: 20.3x\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!zip -r bitnet-checkpoint.zip /kaggle/working/checkpoints/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T14:10:13.355773Z","iopub.status.idle":"2025-08-17T14:10:13.355996Z","shell.execute_reply.started":"2025-08-17T14:10:13.355892Z","shell.execute_reply":"2025-08-17T14:10:13.355902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/bitnet-checkpoint.zip /kaggle/working/checkpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}